
\section{Handling data gaps}
\label{sec:rnn_gen}

If we naively pass a light curve with missing values through the RNN, then varying time intervals will be ignored and false detections could be triggered around gaps. In this research we evaluate, among others, different gap filling approaches to deal with this problem. Since the RNN offers the possibility to make predictions at every time step, we explore the ability of the RNN to predict missing flux values when applied to a light curve with gaps, similar to \cite{morvan2020detrending}. In other words, we extend the RNN and train it to predict subsequent flux values, in parallel to classifying data points as signal or non-signal. To do so, we apply a second FC-network to the outputs of the recurrent layers, to produce an output $\hat{x}_{ij}$ for each data point $x_{ij}$. We use the same amount of layers and hidden nodes for this FC-network as for the one used for the binary classification of data points. The only difference is that the output is not passed through the sigmoid function, because the preprocessed flux can have any value in $\mathbb{R}$. We use an additional loss term so the generative network is trained to improve its predictions such that $\hat{x}_{ij}$ gets closer to $x_{ij}$:
\begin{equation}
    \mathcal{L}_{\text{MSE},i} = \frac{1}{N}\sum^{N-1}_{j=0} (x_{ij} - \hat{x}_{ij})^2.
\end{equation}

\noindent The loss over light curve segment $i$ for the generative RNN then becomes:
\begin{equation}
    \mathcal{L}_i = \mathcal{L}_{\text{BCE},i} + \lambda \mathcal{L}_{\text{MSE},i}.
\end{equation}

\noindent When this network is applied to a light curve with missing data, we replace a missing value $x_{ij}$ = NaN with the predicted value $\hat{x}_{ij}$ and use it as input to the network instead. The generative RNN as described in this section is used in Section \ref{sec:preprocessing} in comparison to other gap-filling approaches.