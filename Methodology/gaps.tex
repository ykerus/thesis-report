
\section{Handling data gaps}

If we naively pass a light curve with missing values through the RNN, then varying time intervals will be ignored and false detections could be triggered around gaps. One way to deal with this problem is to feed the time differences between measurements to the RNN. However, this extra stream of data might increase the chance of overfitting \todo{elaborate}. Other methods include data imputation, for example by filling missing data with linearly interpolated values.

Since the RNN offers the possibility to make predictions at every time step, we explore the ability of the RNN to predict missing flux values when applied to a light curve with gaps. In other words, we extend the RNN and train it to predict subsequent flux values, in parallel to classifying data points as signal or non-signal. To do so, we apply a second FC-network to the outputs of the recurrent layers, to produce an output $\hat{x}_{ij}$ at each data point $x_{ij}$. We add an additional loss term, so the the generative network is trained to improve its predictions such that $\hat{x}_{ij}$ gets closer to $x_{ij}$:

\begin{equation}
    \mathcal{L}_{\text{MSE},i} = \frac{1}{N}\sum^{N-1}_{j=0} (x_{ij} - \hat{x}_{ij})^2.
\end{equation}

\noindent The loss over a light curve $i$ for the generative RNN then becomes:

\begin{equation}
    \mathcal{L}_i = \mathcal{L}_{\text{BCE},i} + \lambda \mathcal{L}_{\text{MSE},i}
\end{equation}

\todo{add weight decay}

\noindent When this network is applied to a light curve with missing data, we can replace a missing value $x_{ij}$ = NaN with the predicted value $\hat{x}_{ij}$ and use it as input to the network instead.