
\section{Simulated data}
Data from missions such as Kepler and TESS are publicly available. Light curves from these missions are, however, difficult to work with from scratch. They contain data gaps, outliers, measurements at slightly varying time intervals. More importantly, they could contain undetected transit signals, or noisy patterns falsely identified as transit candidates. Using only these light curves for the development and validation of our method is therefore undesirable. For the purpose of this thesis, we therefore mostly use simulated data, as these provide us with a ground-truth, and full control over parameters. 

Two sources of simulated data are used, the first of which was produced specifically for this thesis. For clarity we refer to our simulator as ``LCSim'' (Light Curve Simulator) in the following. The second source is the openly available Lilith-4 data set\footnote{\url{https://archive.stsci.edu/missions-and-data/tess/data-products/lilith-4}}, produced by the Lilith data simulator of the TESS pipeline.

\subsection{Generating artificial light curves}
In order to generate realistic data, one needs to consider the components and challenges described in Section \ref{sec:challenges}. For the development and evaluation of new algorithms however, it is convenient if certain features can easily be included or excluded. The LCSim simulator is built such that features such as stellar variability can be turned on and off, or even tuned. In the following, all components are described that make up the LCSim light curves, as well as the distributions used for sampling stellar and planetary parameters. Note that the aim of this part of the work was not to design the most realistic simulator, but rather to be able to generate data that is realistic enough for the analysis and comparison of new signal detection algorithms. The code is made available\footnote{\url{https://github.com/ykerus/transit-detection-rnn}}, in part to encourage the development of new algorithms for transit detection.

\subsubsection{Cadence}
In line with TESS data, we adopt a 2-minute cadence for all simulations in this work. This means that the sampling interval of flux values, or observations, is 2 minutes. For other work, one may choose a cadence of 30 minutes, in order to mimic Kepler light curves better. For more realistic time intervals, one could copy the time arrays of real-world light curves and pass them through LCSim, which will overlay each time step with simulated flux values.

\subsubsection{Stellar variability}

\red{[TODO: improve overview and celerite intuition]}

The physics of stars is still an active region of science. Stars are complex (see \ref{sec:challenges}), so they are not straightforward to model. Gaussian processes (GPs) are used several times in literature to account for background patterns in light curves, both in the context of characterization and simulation of the stellar activity \citep{barros2020improving, zucker2018shallow}. Given their ability to simulate quasi-periodic and stochastic behaviour also observed in stars, we use GPs to account for the stellar variability in our simulated data.

Similar to a Gaussian distribution, which is defined by a mean and a standard deviation or covariance matrix, a GP is defined by a mean function and a covariance function, or kernel. In our case, we define the mean function to be 1 at each time step, so the behaviour of the GP is fully determined by its kernel. We can get an instance of the function representing stellar variability by taking a sample from the multidimensional Gaussian distribution defined by our mean and covariance function. Sampling multiple times from the same distribution will give us unique samples. However if we use the same kernel for each of the samples, then they will still have the same properties in terms of variability, e.g. amplitudes and timescales. To generate truly unique samples, we define a kernel for each light curve separately. This can be a costly process, as for each light curve consisting of 20000 data points, a naive approach would require the construction of a kernel with $20000 \times 20000$ entries, which would be around 3 GB to store in memory. 

Instead, we make use of "celerite" \cite{foreman2017fast} to construct kernels and sample from GPs. celerite allows for efficient implementation of GPs and is optimized for one-dimensional astronomical data. The kernel we use for each light curve consists of two terms: one for simulating the short-term variability due to granulation, and one for long-term variability due to rotation modulation. The former is a kernel representing a stochastically-driven damped harmonic oscillator (SHO). The SHO term has corresponding parameters $\rho$ for the undamped period of the oscillator, $\tau$ for the damping timescale of the process and $\sigma_\text{SHO}$ for the standard deviation of the process. \red{[TODO: give formulas and meaning of other parameters such as $Q$, which are implicitly defined by the above three parameters]}. A value $Q=1/\sqrt{2}$ is commonly adopted to describe the process of granulation (e.g. \cite{barros2020improving}), which we also use in this work. We can then write: \red{[TODO: formula rewriting]}, which leaves us with $\rho$ and $\sigma_\text{SHO}$ to specify. A mixture of two SHO terms can be used to account for stellar rotation, which we use as second term of our kernel. This term is defined by five parameters:  $\sigma_\text{rot}$ indicating the standard deviation of the process; the period of rotation; $Q_0$ which represents the quality factor for the secondary oscillation; $\text{d}Q$ for the difference between quality factors of the two modes; and $f$ indicating the fractional amplitude difference between the two modes. We will loosely refer to $\sigma_\text{rot}$ as the amplitude of stellar rotation.

\subsubsection{Photon noise}
To simulate photon noise, we simply sample from a Gaussian distribution $\epsilon_i \sim \mathcal{N}(0, \sigma)$ and add $\epsilon_i$ to the corresponding flux at time step $i$. The standard deviation $\sigma$ defines the level of time independent noise in the light curve, and is slightly different for different light curves.

\subsubsection{Transit signals}
The most important ingredient of our data is the transit signal. Fortunately, several tools exist to simulate signals from transiting exoplanets. One of these is “batman” \citep{kreidberg2015batman}, which is based on the equations formulated by \cite{mandel2002analytic}. Given a set of planetary and stellar parameters, batman computes the transit light curve, which is equal to the fraction of observed starlight computed at each time step. This light curve is a idealized light curve, assuming no noise and no stellar activity. We therefore only need to multiply this with our noisy background to get the desired light curve with transit signals. 

The parameters used for the simulation of transit light curves are: epoch $t_0$, or the transit time of the first transit in the light curve; the orbital period $P$ of the planet; the planet’s radius relative to its host star (radius-over-radius or \textit{ror}, e.g. for Earth, \textit{ror} = 0.01); orbital parameters, such as semi-major axis $a$, inclination $i$ and eccentricity \textit{ecc}; and limb darkening parameters of the host-star. We approximate the limb darkening of stars with a quadratic function, parametrized by two coeffients $u_1$ and $u_2$ [TODO: give function]. We constrained the respective values of $P$ and $a$ using Kepler’s Third Law:
\begin{equation}
    \label{eq:kepler}
    P^2 = \frac{4 \pi^2}{GM}  a^3.
\end{equation}

In this formula, $G$ is the gravitational constant ($=6.67408 \cdot 10^{-11} \text{m}^3 \text{kg}^{-1} \text{s}^{-2}$), and $M$ is the star’s mass. We thus get the extra parameter $M$ that we need to specify. Furthermore, $a$ is expressed in terms of stellar radii $R$, which in turn needs to be specified in order to compute Equation \ref{eq:kepler}.

The orbital inclination is assumed to be 90$^\circ$ for all planets we simulate, meaning that each planet moves across the center of the stellar surface in our line of sight while it orbits. In reality, the inclination can take any value between 0$^\circ$ and 180$^\circ$. However, as described in \ref{sec:challenges}, only a few values result in a transit signal, which also makes the evaluation of detections more difficult. To avoid these problems, we therefore simply set $i = 90^\circ$.

\subsubsection{Multiple planets}
A single star can have multiple orbiting planets. LCSim allows for the simulation of arbitrarily many orbiting planets, albeit ignores whether a certain configuration of planets is realistic or not. In case a light curve is required to have transit signals from two different planets, the transit simulator is simply called twice with different parameters for the planets while maintaining all the parameters belonging to the star. The result might be that the two planets coincidentally have the same distance from their host star, which is unrealistic but does not pose a problem for the aims of this thesis. 

Overlapping transit signals, on the other hand, might make the process of developing a detection algorithm more difficult. If overlapping signals have been found, one needs to make sure which signal triggered a detection: it could be one of the two, or both. Since overlapping signals are far less common in real-world data than non-overlapping signals, we only simulate non-overlapping transit signals in this work to avoid confusion.

\subsubsection{Parameter sampling}
For each light curve, we need to specify 8 parameters to define the background variability. For each transiting planet we need to specify 3 parameters, with an additional 4 for the first planet to define necessary relations with the host star. The distributions used to sample each parameter are given in Table \red{[TODO: table]}. These distributions are based, partly on previous literature (see table footnotes), and partly on a subset of TESS light curves for which parameters such as stellar amplitude, white noise, were estimated and parameters such as radius were given. We took inspiration from the set of TOIs (TESS Objects of Interest) and confirmed exoplanets to define parameter ranges for the transiting exoplanets.

\subsection{Lilith-4}
In addition to the LCSim light curves, we use light curves generated by the Lilith simulator of TESS' pipeline to evaluate and compare our method to other detection algorithms. This data brings us close to real-world data, but still comes with a ground-truth. The Lilith-4 data set comprises four sectors of simulated TESS data, and takes into account readout errors, spacecraft jitter, focus errors, diffuse light, cosmic rays, stellar variability, transiting exoplanets, eclipsing binary stars, and more \citep{osborn2020rapid}. We use this data for several reasons. First, it functions as a test for both our algorithm and the LCSim simulator. Unexpected results would lead us to believe that either our algorithm is too dependent on the data that is used, or that LCSim data is too unrealistic. Second, it allows the evaluation of preprocessing steps in combination with our algorithm, that would be necessary in the case of using real-world data. Lilith-4 also includes information about the pointing of telescope within the so-called centroid data, which can be used by our algorithm to potentially benefit from.