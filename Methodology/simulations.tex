
\section{Data simulations}

For the development and evaluation of our RNN-based algorithm we used simulated data. This is because simulated data comes with an absolute ground-truth of the hidden signals, and for the development of the algorithm it proved useful to be in control over the parameters of the input light curves. In contrast, real-world data cannot be changed, and in some cases the ground-truth is ambiguous. Realizing that a single source of simulated data may limit our results, two sources of data were used. First, our Light Curve Simulator, or LCSim, was specifically designed for this work. Second, we made use of simulated light curves in the Lilith-4 data set, which was produced by the Lilith data simulator of the TESS pipeline. The following subsections describe the details of the simulations, the data sets used in this work, and some general preprocessing of the data.

\subsection{LCSim}
\label{sec:lcsim}

In line with TESS data, we adopt a 2-minute cadence for all simulations in this work. The simulator is made available here\footnote{\url{https://github.com/ykerus/transit-detection-rnn}} and may also be used to generate light curves with other measurement intervals.

Stellar variability is simulated using Gaussian processes (GPs). GPs have been used several times in literature to account for background activity in light curves, both for characterizing and simulating stellar activity \citep{barros2020improving, zucker2018shallow}. GPs are defined by a mean and a covariance function, or kernel. In our case, the mean function is always 1, so the behaviour is fully determined by the kernel, which should therefore hold all of the star's relevant properties. An instance of the function representing stellar variability can be obtained by sampling from the multi-dimensional Gaussian distribution that is defined by the mean and covariance function. However, if we use the same kernel for each sample, then each light curve has the same underlying stellar properties. Therefore, we construct a kernel for each light curve. To construct a kernel that is able to simulate quasi-periodic behaviour as observed in stars, we make use of the Python library \texttt{celerite} \citep{foreman2017fast}, also used by e.g. \cite{ment2020toi} and \cite{barros2020improving}.  \texttt{celerite} has built-in kernels to simulate stellar granulation and rotation modulation. These kernels are specified by the standard deviations and time scales of the processes, among several other parameters which are given in Table \ref{tab:params}. The two oscillation terms describing stellar rotation are partly defined by $Q_0$, $\text{d}Q$ and $f$ which respectively describe the quality factor of the secondary oscillation, the difference between quality factors of both oscillation modes, and the fractional amplitude of the secondary mode compared to the primary (see the documentation\footnote{\url{https://celerite2.readthedocs.io/en/latest/api/python/}. Accessed: 18-08-2021.}). $P_{\text{rot}}$ and $\sigma_{rot}$ describe the primary period of the rotation and the standard deviation of the variability respectively. For the granulation term, we have $P_\text{gran}$ and $\sigma_\text{gran}$ to describe the period and standard deviation, and $Q$ the quality factor of the oscillation. For granulation it is common to choose $Q=1/\sqrt{2}$ \citep{barros2020improving}.

To simulate photon noise, we sample from a Gaussian distribution $\epsilon_{ij} \sim \mathcal{N}(0, \sigma_i)$ and add $\epsilon_{ij}$ to the corresponding flux at time step $j$ in light curve $i$. The standard deviation $\sigma_i$ ($\sigma$ in Table \ref{tab:params}) defines the level of time-independent noise in the light curve, and is specified for each light curve separately. 

For the simulation of transit signals, we made use of the Python library \texttt{batman} \citep{kreidberg2015batman}, which is based on the equations from \cite{mandel2002analytic} which describe the physics of transits. We approximate the stellar limb darkening effect with the built-in quadratic function, parametrized by $u_1$ and $u_2$. The values for these parameters were sampled using intermediate parameters $q_1$ and $q_2$ such that they always produce physical transit signals, according to \cite{kipping2013efficient}. We constrained the orbital period $P$ of a planet and its semi-major axis $a$ according to Kepler's Third Law (see Section \ref{sec:transit}). In order to do so, we also specify the stellar mass $M$ and radius $R$ so the result is compatible with the input requirements of \texttt{batman}. The orbital inclination $i$ is assumed to be 90$^\circ$ for all planets we simulate, to avoid problems with non-existent transit signals, or transit depths that are more difficult to predict. Setting $i=90^\circ$ ensures that a simulated planet moves in front of the stellar disk during transit. The eccentricity $ecc$ is sampled from a Beta distribution, following \cite{kipping2013parametrizing}. The radius of the planet is specified by $ror$, which is the fractional radius of the planet relative to its host star. Lastly, $w$ is specified to indicate in which part the exoplanet is in its orbit when it transits its host star. If $ecc=0$, then $w$ has no effect, otherwise $w=90^\circ$ will correspond to the shortest possible transit duration, and $w=270^\circ$ to the longest.

In case a light curve is required to have transit signals from multiple different planets, the transit simulator is simply called iteratively with different parameters for the planets while maintaining all the parameters belonging to the star. The result might be that the two planets coincidentally have the same distance from their host star, which would be unrealistic, but does not pose a problem for the purpose of this thesis. Overlapping transit signals, on the other hand, could make the process of developing and evaluating our detection algorithm more difficult. If overlapping signals have been found, one needs to make sure which signal triggered a detection: it could be one of the two, or both. Since overlapping signals are far less common in real-world data than non-overlapping signals, we only simulate non-overlapping transit signals in this work to avoid confusion.

Several data sets used in this work are based on LCSim. LCSim-1500 is a set of light curve segments consisting of $N=1500$ data points per light curve, which is used for training and evaluating the network. These light curves are supposed to imitate light curve segments that are obtained from splitting an original full-length light curve into parts. However, with LCSim we simulated the segments directly to reduce computational costs, so we do not have access to the ``original'' light curves. LCSim-500 is similar, only it has fewer ($N=500$) data points per light curve. Both data sets contain 15000 training samples, 5000 validation samples and 5000 test samples. Half of the samples per split contains no transit signals, 35\% contains a single transit signal and 15\% two transit signals. For the evaluation of our method's ability to retrieve signals in full-length light curves, we use LCSim-Mono and LCSim-Single. Both data sets consist of 5000 light curves spanning 27.4 days (i.e. 19728 data points per light curve). 50\% of the samples in LCSim-Mono contain a single transit signal, and 50\% of the samples in LCSim-Single contain at least three transit signals from a single transiting planet. None of these data sets contain data gaps. However, to evaluate the effect of gaps and different gap-handling approaches, we use LCSim-1500-Gap, which is LCSim-1500 with injected gaps. Zero, one or two large gaps between 2 and 10 hours are injected with 50\%, 35\% and 15\% probability respectively. In addition, a random selection of 2\% of the data points is removed. For the training of the representation RNN, we generated pairs of white-noise dominated light curves with the same distribution of transit signals as for the other data sets. 50\% of the samples was a positive pair, and 50\% a negative pair, as explained in \ref{sec:rnn_repr}.

Each light curve is directly simulated as median normalized, i.e. centered around 1. Prior to applying the RNN, we always center the inputs around zero by subtracting 1, and standardize the entire data set. To do so, for each data point we subtract the mean and divide by the standard deviation over all data points in the training split. In Chapter \ref{chap:experiments}, we experiment with additional preprocessing steps.

%\cdashlinelr{1-4}


\begin{table}[]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
                      &  Parameter               & Sampling distribution or relation                           & Units            \\ \midrule
Photon noise          & $\sigma$        & $\exp(\text{Uniform}(\log(0.0005), \log(0.003)))$           & -                \\ \cdashlinelr{1-4}
\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}Stellar\\ rotation\end{tabular}} &
  $Q_0$ &
  $\text{LogNormal}(0, 2)$ &
  - \\
                      & d$Q$            & $\text{LogNormal}(0, 2)$                                    & -                \\
                      & $f$             & $\text{Uniform}(0.1, 1)$                                    & -                \\
                      & $P_{rot}$       & $|\text{Normal}(5,2)| + 1$                                  & days             \\
                      & $\sigma_{rot}$  & $\exp(\text{Uniform}(\log(0.0003), \log(0.005)))$           & -                \\\cdashlinelr{1-4}
\multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}Stellar \\ granulation\end{tabular}} &
  $\nu$ &
  $\text{LogNormal}(4.5,1) \cdot 10^{6}$ &
  Hz \\
                      & $P_{gran}$      & $1/\nu/86400$                                               & days             \\
                      & $\sigma_{gran}$ & $\text{LogNormal}(\log(\num{2e-6} \cdot \nu^{-0.61}), 0.1)$ & -                \\
                      & $Q$             & $1/\sqrt{2}$                                                & -                \\ \cdashlinelr{1-4}
\multirow{5}{*}{Star} & $M$             & $|\text{Normal}(0.9, 0.25)| + 0.1$                          & $\text{M}_\odot$ \\
                      & $R$             & $|\text{Normal}(M^{1/3}-0.1, 0.2)| + 0.1$                   & $\text{R}_\odot$ \\
                      & $q_1$, $q_2$    & $\text{Uniform}(0, 1)$                                      & -                \\
                      & $u_1$           & $2 q_2\sqrt{q_1}$                       & -                \\
                      & $u_2$           & $(1-2q_2)\sqrt{q_1}$                                        & -                \\\cdashlinelr{1-4}
\multirow{5}{*}{Exoplanet} &
  $P$ &
  $\exp(\text{Uniform}(\log(P_{min}), \log(P_{max})))$ &
  days \\
 &
  $a$ &
  $( (M \cdot \text{M}_\odot)\cdot 86400P \cdot G / (2\pi)^2 )^{1/3} / (R\cdot\text{R}_\odot)$ &
  $R$ \\
                      & $ror$           & $\exp(\text{Uniform}(\log(0.02), \log(0.15)))$              & -                \\
                      & $ecc$           & $\text{Beta}(0.867, 3.03)$                                  & -                \\
                      & $i$             & $90$                                                        & deg              \\
                      & $w$             & $\text{Uniform}(0, 360)$                                    & deg              \\ \bottomrule
\end{tabular}
\label{tab:params}
\caption{Sampling distributions for the parameters used in LCSim (see main text for their descriptions). Inspiration for these distributions came from online tutorials, e.g. from \texttt{exoplanet} \citep{exoplanet:joss} for  $Q_0$, d$Q$ and $f$; literature, e.g. \cite{martins2020search} for stellar rotation, \cite{kallinger2014connection} for granulation, \cite{kipping2013efficient} and
\cite{kipping2013parametrizing} for efficient sampling of $ecc$, $u_1$ and $u_2$; and parameter values from known transiting planets and real-world light curves from TESS \citep{ricker2014transiting} for remaining planetary and stellar parameters and photon noise. $\text{M}_\odot = \num{1.989e30}$ kg and $\text{R}_\odot = \num{6.963e8}$ m are the solar mass and radius, $G$ is the gravitational constant, and $P_{\text{min}}$ and $P_{\text{max}}$ a predefined minimum and maximum orbital period of the planet. Note that the relations between parameters are in some cases not realistic. For example, $M$ and $R$ are only sampled to get a reasonable and slightly random relation between $P$ and $a$, but themselves have no other function in the simulation. However, for the purpose of our research this is no problem as we do not make explicit use of these parameters in our detection algorithm, and the transit signals are still defined by a complex interplay between the parameters specified above.}
\end{table}


\subsection{Lilith-4}
\label{sec:lilith-4}

The Lilith-4 data set comprises four sectors of simulated TESS data, and takes into account readout errors, spacecraft jitter, focus errors, diffuse light, cosmic rays, stellar variability, transiting exoplanets, eclipsing binary stars, and more \citep{smith2019four}. This simulated data set is therefore expected to be considerably more realistic than ours, which is why we include it in this research.  Furthermore, it allows for the evaluation of certain preprocessing steps in combination with our algorithm, that would be necessary in the case of using real-world data. Lilith-4 also includes information about the pointing of the telescope, the centroid data, which can be used by our algorithm to potentially benefit from. The data can be accessed here\footnote{\url{https://archive.stsci.edu/missions-and-data/tess/data-products/lilith-4}. Accessed: 18-08-2021.}, as well as the data validation (DV) files, which describe the transit candidates detected by the TESS pipeline.

We prepared two data sets based on Lilith-4. As explained in Section \ref{sec:astro_false_pos}, we excluded light curves with (B)EB signals. The first data set, Lilith-1500, is similar to LCSim-1500. Lilith-1500 is obtained from median normalizing the light curves that only span a single sector in Lilith-4, and subsequently splitting them into equally sized segments of $N=1500$ data points. Missing data points are replaced with NaN values, and slight deviations from a 2-minute cadence are corrected for. The segments were chosen to be partially overlapping, so we have more data to train the network. Segments are rejected if they contain transit signals which are less than half visible, overlapping, less than 1 or more than 13 hours in duration, or have a depth $\delta$ relative to $\sigma$ (i.e. $\delta/\sigma$) of less than 0.25 or more than 10. For Lilith light curves, $\sigma$  is not given, so we estimate it by flattening the input with a sliding median filter with a window size of 30 minutes, and subsequently taking the standard deviation. From a total of 68236 segments, 61\% was used for training, 22\% for validation and 17\% for testing. Segments from the same light curve were kept in the same split, to avoid overlap between training and test data. To evaluate the ability of our algorithm to detect planets in Lilith light curves, we prepared Lilith-Multi. This data set consists of 6511 light curves which were excluded from Lilith-1500, 2670 of which contained at least three transit signals from at least one planet. We used light curves of each sector separately, so we did not stitch light curves from targets which had observations over multiple sectors. Only planets with at least three transit signals in the same light curve with relative transit depths $0.25 < \delta/\sigma < 10$ were used in the evaluation. Regarding preprocessing, we take the same steps as for LCSim data. Additionally, we filtered out flagged low-quality data points, using the flags explained by \cite{tenenbaum2018tess}.

