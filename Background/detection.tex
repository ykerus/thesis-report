
\section{Transit signal detection methods}

\comment{from here onwards up to Methodology, the text was written a while ago. I am satisfied with the first few sections, but from section 2.5 (RNNs) I need to change/add a lot still.}

\todo{write better section introduction}
When Kepler or TESS data are released, both the data and the initial analyses by their data processing pipelines are released. These analyses include the detection of potential transit signals. In the following, we describe how the pipeline detections come about and discuss several other approaches that have been proposed in literature to detect transit signals.

\subsection{Least squares: BLS and TLS}

The Box Least Squares (BLS) algorithm, proposed by \cite{kovacs2002box}, is commonly adopted for transit signal detection \todo{cite examples}. The transit signal is approximated by a two-level signal with a maximum of, say 1, and a minimum of $1-\delta$ in units of normalized brightness, or flux. This box signal is parametrized by its depth $\delta$, duration, period and reference time, or epoch. The search is performed by trying many values for each of these parameters, and finding minima in the squared error between the fit and the data. In order for this to work well, the light curve is expected be detrended prior to the search. Subsequently, a detection can be defined using a threshold on the signal-to-noise ratio (SNR) of the fit, or other metrics. The SNR of a transit event would intuitively be defined as $\text{SNR} = \delta / \sigma_w \cdot \sqrt{n_t}$, with $n_t$ the number of measurements that belong to the transit signal and $\sigma_w$ the estimated white noise in the detrended light curve. 
    
However, according to \cite{pont2006effect} the time dependent ``red'' noise in light curves also affects transit detection. This red noise includes, for example short scale stellar activity such as granulation, which could mimic transit signals. They redefine the SNR of a transit signal by including both the white noise and the time dependent noise on the time-scale of the transit duration, and provide an algorithm to estimate these values. Their results show that a detection threshold for BLS can be set better if red noise is accounted for.

In later years, variants to the BLS algorithm were proposed. For example, \cite{carter2013quasiperiodic} relax the assumption of strictly periodic signals, and use a quasi-periodic transit function that is matched with the data.

More recently, the Transit Least Squares (TLS) algorithm was propsed \citep{hippke2019optimized}, which is similar to BLS in that it fits a function to the data by trying many parameter settings. As opposed to BLS, TLS takes into account the effect of limb darkening on the transit shape, and is therefore more expressive than BLS. For this reason, TLS generally performs better at detecting transit signals than BLS, however at the cost of longer computation times.  

Both BLS and TLS require the light curve to be detrended prior to the search, which can be seen as a downside to the simplicity of both algorithms. Their performance therefore greatly depends on the way the input light curve is detrended. \cite{hippke2019wotan} compare a wide range of detrending methods, before applying TLS to search for transits. They evaluate each method by the extend to which known transit signals are retrieved by the TLS algorithm, after applying the detrending method. The methods evaluated include sliding mean and median filters, Gaussian processes, the Savitzky-Golay filter which is used as default by the light curve processing library Lightkurve, and more. The authors show how detrending could significantly alter or reduce the transit signals, for some methods more than other. Furthermore, for window-based filters, it was found that a window size of three times the transit duration works best, but that is generally not known beforehand.


\subsection{Simultaneously modelling background and signal}
In order to avoid the risks that are involved with detrending, one could simultaneously model background stellar activity with the transit signals. This approach was taken by \cite{foreman2015systematic}, where each light curve was described as a combination of the 150 most representative components of the principle component analysis (PCA) of K2, while simultaneously fitting for a box-shaped transit model. Since the background is modelled side-to-side with the transit signal, the background model is less prone to overfit to the transit signal, and the transit signal is therefore expected to stay better intact.

On the other hand, \cite{kovacs2016periodic} argue that with simultaneous modelling, there are more degrees of freedom and thus more chance of false positives. They found better results by first detrending the light curve, and subsequently searching for transit signals. This approach was also found to be considerably more efficient.

\subsection{TESS and Kepler pipeline}
Although the pipelines of TESS and Kepler cover the processing of raw images to systematics corrected light curves, we focus only on the method that is used to search for transit events.  The TESS pipeline is largely based on the Kepler pipeline \citep{jenkins2016tess}, so we base our description of both pipelines on the Transiting Planet Search (TPS) module as described in the Kepler Data Processing Handbook \cite{jenkins2017kepler}. 

In the TPS module, the input light curve first undergoes a series of preprocessing steps, e.g. stitching different sectors of data together and filling gaps. Subsequently, a time-varying whitening filter is applied, so the light curve is transformed to the time-frequency domain. This pre-whitening filter is supposed to clear the light curve of time dependent, or colored, irrelevant noise. The use of a pre-whitening filter for transit search has been adopted more often, e.g. \cite{carpano2003detecting}, as it is considered to be the optimal detector in combination with a simple matched filter in the case for colored Gaussian noise, as explained in \cite{jenkins2002impact}. However, \cite{rodenbeck2018revisiting} note that a pre-whitening filter can introduce features that could be misinterpreted as signal. The pipeline therefore removes positive flux outliers which could introduce transit-like features in the whitened flux. Since the whitening filter can also alter transit signals, the transit pulse train that is used as a match filter is whitened by the same filter. After single events have been determined, a grid of periods, durations and epochs is searched through to find a least squares fit to the signal. A reprentative limb darkening model is used to better match the trial signal with the data.
[TODO: describe differences TCE and TOI and KOI]

\subsection{Other classical detection methods}
In cases where a periodic signal resembles a sine function, the Fourier transform (FT) provides a good way to detect it. For the transit signal this is in general not the case as its duration is short relative to its period. For ultra-short-period ($< 1$ day) planets on the other hand, the duration becomes larger relative to the period, and the FT will have similar detection performance as the BLS algorithm. \cite{sanchis2014study} use a Fourier-based method to detect transits from ultra-short-period planets in Kepler data.
They argue that the FT produces less disturbing harmonics in its resulting spectrum compared to BLS, but BLS is more effective for longer period planets. 
    
Another approach to transit detection is by using the dispersion of the phase folded light curve. \cite{plavchan2008near} fold a given light curve over several trial periods, each time evaluating the difference between the folded light curve and its boxcar-smoothed counterpart. A small set of periods for which the folded curve corresponds best to the shoothed phase curve, is further analysed for transits.
   
\cite{wheeler2019weird} also propose a method which aims to minimize the phase dispersion by folding the light curve at different trial periods. Since this method does not assume any specific signal shape, it is sensitive to strictly periodic signals of arbitrary shape. Their method was applied by \cite{chakraborty2020hundreds} to find 377 previously unreported signals in the first year of TESS observations.


\subsection{``Intelligent'' algorithms}
The human eye can function as an excellent tool for pattern recognition and can sometimes exceed algorithms in complex ways.  For this reason, a citizen science project was launched, called Planet Hunters. A participant, or user, is presented with a light curve from Kepler and is asked to flag any part of the light curve that resembles a transit signal. After six months since the launch, millions of classifications were made and later \cite{fischer2012planet} reported the detection of first two planet candidates that were flagged by participating volunteers. We can view this process as a detection algorithm: a pipeline corrected light curve is presented to the user, say detector, which has prior knowledge about the shape of the signal that we are interested in. Without requiring the light curve to be detrended or folded, individual events are flagged by the detector. If there is enough confidence about a certain signal, i.e. enough users have flagged the same events, the `detection' is passed to the vetting stage. For Planet Hunters, this last stage consists of experts in the field who rule out clear false positives and conduct follow up observations to confirm the planets. 

Similarly, one could use artificial intelligence for the task of transit detection, as proposed by \cite{pearson2018searching}. In this work, a one-dimensional convolutional neural network (1D-CNN) was trained to classify light curve segments either as containing a transit signal or not. On its own, this approach could be categorized as identification method, as their model can only be applied to small inputs of fixed size (180 data points), and provides a single binary output for a given input. To use the 1D-CNN for detection, two approaches are proposed. In the first approach, the model is applied to the whole light curve by using a sliding window, to obtain a prediction at each point in time which is referred to as the probability time series (PTS). Subsequently, the average distance between peaks in the PTS can give an estimate of the periodicity of the signal. In the second approach, the light curve is folded over a given period, after which the sliding window approach is applied to the resulting phase curve. A detection in the phase curve directly gives an estimate of the periodicity, but this approach requires a brute-force search over period values to find the one for which the signals overlap in the phase curve.

Extending on this work, \cite{chintarungruangchai2019detecting} present a similar method. However, in their approach the light curve is folded, such that the result is not one, but two dimensional. This is accomplished by stacking each part of the folded light curve that would normally be interleaved. Subsequently, they train a 2D-CNN to classify these ``images'' as signal or non-signal. The search for a signal still requires trying different values for the period over which the light curve is folded, but this methods should be more robust to small errors between the trial and the true period.

\cite{zucker2018shallow} test the feasibility of using 1D-CNNs for transit signal detection in comparison with the BLS algorithm. They use simulated data for which Gaussian processes are used to model stellar variability. For a given input light curve, the tested methods both output a single binary value (signal, non-signal), which was used for the analysis of their performance. However, no comparison was made between BLS and the 1D-CNN in their ability to explicitly detect a transit signal from a given planet. In other words, the question remains whether a CNN-based algorithm could compete with the BLS algorithm if the task was to output the period $P$ and epoch $t_0$ of a detected signal. These values allow one to know exactly which signal was detected and when its repetitions can be expected in the future.


% \subsection{Bayesian approach}
% \cite{aigrain2002bayesian} present an approach in which the likelihood of the data is calculated given a set of parameters. Parameter values are searched to find a setting for which the probability is maximized according to Bayes' theorem. 
