
In order to better understand and evaluate the individual components of our algorithm, the first sections of this chapter are devoted to take a closer look at each one of them. We compare the RNN with the more commonly used CNN in a binary classification task for input light curve segments, and illustrate how the RNN naturally extends to full-length light curves. In the same section, Section \ref{sec:exp_hybrid}, we also motivate the network architecture that was used throughout the rest of the experiments. Since preprocessing can be of influence on the performance of the network, in Section \ref{sec:exp_preprocessing} we compare the effect of several different preprocessing steps on training of the network. For example, several approaches to dealing with gaps are evaluated, as well as the use of a generative network which is trained to fill gaps by itself.
Section \ref{sec:exp_algorithm} illustrates the workings of both detection algorithms described in Section \ref{sec:algorithms}, where success and failure cases are identified. In addition to these algorithms, we illustrate how the network's representations of each time step can be used to potentially resolve ambiguities in the matching of individual signals when searching for periodicities. 

In the last three sections, we evaluate our method as it would be used in real-world applications. First, we show that the RNN has a large potential in the task of monotransits detection. Subsequently, we compare our RNN-based detection algorithm with the BLS algorithm in the task of detecting single planets in LCSim light curves. Lastly, in the task of detecting arbitrarily many planets in more realistic light curves from Lilith-4, we compare the detections from the TESS pipeline, BLS and our method. 