
\section{Limitations and suggestions}


The main limitation of this work is that we used simulated data. Simulated data was used to aid in the development and evaluation of our method,  because they allow for full control over parameters and come with a ground-truth.  However, we cannot say based on our experiments whether the results will hold when real-world data is used. We expect that real TESS data is similar to Lilith data, and therefore do not expect to observe large differences when the RNN would be applied to real-world light curves.  We did observe differences between the results obtained using LCSim data and Lilith data. One of the differences was the overall lower recall of both BLS and the RNN, when applied to Lilith light curves instead of LCSim light curves. We believe that the main reasons for this are the fact that Lilith data contained relatively more shallow transit signals than LCSim data, and for the RNN that the absence of noise in the PTS for Lilith light curves prevented planets to be detected at low detection thresholds. 

Related to the fact that we used simulated data, is our choice to leave out signals from eclipsing binary (EB) systems in this work. When applying the detection method to real-world data, this choice cannot be made, because we do not know beforehand whether an EB signal is present in a given light curve. Our current approach however, can still be applied to  light curves containing EB signals. In case the EB signal resembles a transit signal, the RNN might treat it as transit signal and output high values. Other algorithms would likely also make mistakes if the EB signal resembles a transit signal. In case the EB signal is stronger than the typical transit signal, the RNN might treat it as noise, because it has never seen such signals during training. However, its outputs might also become unreliable for the same reason. Alternatively, one could include known EB signals during training, and treat them as separate class of signal. The network could then be trained for the three-class classification problem of separating data points belonging to transit signals, EB signals or noise, with corresponding transit-PTS, EB-PTS and noise-PTS.

In this work, we separated two components of our detection algorithm: the RNN and the method used to extract candidate detections from the RNN's predictions. When referring to the detection performance of the RNN, in most cases we referred to the tradeoff that can be made between precision in recall in the task of classifying individual data points as signal or non-signal. However, this measure of performance is not the same as the performance of our method to detect transit events, because transit events cover multiple data points. We tuned our RNN based on the performance measure over individual data points, and the subsequent algorithm based on the true task of providing the correct P and t0 of transit signals. The overall algorithm might be improved if it is tuned from front to end, based on the final ouptuts for P and t0. Such an approach would, however, result in longer computation times since in the worst case, the number of parameter settings to be evaluated is increased exponentially.

Another limitation of this work is our choice of baseline. We opted for box-fitting algorithms, as they are intuitive and relatively efficient compared to other least squares methods. However, the more realistic transit model used in Transit Least Squares (TLS, \todo{cite}) was found by the authors to be match the transit signals in Kepler data in over 99\% of the cases better than a box model. TLS requires priors on the stellar density to perform comparibly with BLS in terms of efficiency. In the case for LCSim, we did not implement a physical relation between stellar density and the limb darkening parameters of the simulated star. Without priors, TLS would have taken up to three times longer than BLS to run in our experiments, which is what motivated us to use BLS over TLS. 

The use of least squares methods compared to the use of a neural network brings us to the problem of interpretability. Although they may be computationally expensive, least squares fitting algorithms with physics motivated transit models are arguably more interpretable than the hidden dynamics of neural networks. First of all, in our attempts of increasing the interpretability of our network, we only looked at the outputs of the network. In other words, the black box remains a black box, simply with more outputs, e.g. confidence values, or representation vectors. Similarly, one could let the RNN output the predicted depth of a signal, which, in combination with estimated duration allows for the approximate SNR to be computed of candidate detections. However, what causes a certain value or vector to be outputted by the model remains hidden from the user. Second of all, which values should be used for setting a detection threshold in the RNN-based algorithm remains a question. In this work, we aimed to come up with a simple detection criterion using the values in the PTS. In our approach we summed the values of the PTS corresponding to the same phase for a given period, while scaling down by the square root of the number of values in the sum. This was a rather arbitrary choice, but showed to be sufficient for the purpose of this work.  Third of all, the output of our algorithm in its current form does not provide uncertainties over the predicted values for P and t0. For the identification step, it would be useful to have access to these, as deviations from the expected time of signals could more easily be explained by imprecise detection results. We expect the PTS alone to be sufficient to allow for some form of uncertainty estimation in the provided parameters, but further research should establish how this can be done best.